{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG & Faiss & LLaMA 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pdf ÌååÏùºÏóêÏÑú string Ï∂îÏ∂ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 11:51:46.009 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/llama3rag/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# pdf ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂úÌïòÍ∏∞ (ÏûÖÎ†• : pdf ÌååÏùº Í≤ΩÎ°ú)\n",
    "uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
    "\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # PDF ÌååÏùº Ïó¥Í∏∞\n",
    "    with pdfplumber.open(uploaded_file) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    \n",
    "    st.write(\"Extracted Text:\", text)\n",
    "    model_input = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llama3rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict() \n",
    "    )\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Function that defines the RAG chain\n",
    "def rag_chain(url, question):\n",
    "    retriever = load_and_retrieve_docs(url)\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "    response = ollama.chat(model='llama3', \n",
    "                           messages=[\n",
    "                                {\"role\": \"system\",\n",
    "                                 \"content\": \"You are a helpful assistant. Check the url content and answer the question. Translate the answer in Korean with emoji.\"\n",
    "                                },\n",
    "                                {\"role\": \"user\", \"content\": formatted_prompt}])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=rag_chain,\n",
    "    inputs=[\"text\", \"text\"],\n",
    "    outputs=\"text\",\n",
    "    title=\"LLAMA 3: RAG Chain Question Answering\",\n",
    "    description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaForCausalLM, LlamaTokenizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Î™®Îç∏ Î∞è ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï¥àÍ∏∞Ìôî\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecapoda-research/llama-13b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pdfplumber\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Î™®Îç∏ Î∞è ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï¥àÍ∏∞Ìôî\n",
    "model_name = \"decapoda-research/llama-13b\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_pdf(pdf):\n",
    "    # PDF ÌååÏùºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú\n",
    "    with pdfplumber.open(pdf) as pdf_file:\n",
    "        text = \"\"\n",
    "        for page in pdf_file.pages:\n",
    "            text += page.extract_text()\n",
    "    \n",
    "    # ÌÖçÏä§Ìä∏Î•º Î™®Îç∏ ÏûÖÎ†•ÏúºÎ°ú ÏÇ¨Ïö©\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Gradio Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï\n",
    "iface = gr.Interface(\n",
    "    fn=process_pdf,\n",
    "    inputs=gr.inputs.File(label=\"Upload PDF\", type=\"file\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"PDF to Text with Llama\",\n",
    "    description=\"Upload a PDF file and get the generated response using Llama model.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "paragraphs.append(pdf_text[:1000])\n",
    "paragraphs.append(pdf_text[1000:2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Strong command-line skills in Linux OS for system administration and software deployment\n",
      "& Web Dev ‚Ä¢ Proficient in Git/Github for streamlined version control\n",
      "‚Ä¢ Experienced in full-stack web development processes using HTML/CSS/Boostrap5/JavaScript/Flask\n",
      "English ‚Ä¢ Fluent in both written and spoken English, confident to engage in international activities\n",
      "& Soft Skills ‚Ä¢ A people person - ensuring happy customers, whilst working well with internal teams\n",
      "Certifications\n",
      "TOEIC (score : 970) Mar 16, 2024 YBM TOEIC\n",
      "OPIC (level : AL) Feb 05, 2024 ACTFL OPIc\n",
      "AWS CCP (Certified Cloud Practitioner) May 04, 2024 AWS (Amazon Web Service)\n",
      "Elementary Teacher Certification 1st : Feb 16, 2022 1st : Seoul Metropolitan Office of Education\n",
      "(1st & 2nd Grade Certified Teacher) 2nd : Feb 24, 2017 2nd : Seoul National University of Education\n",
      "Blogs\n",
      "https://shorturl.at/ Fluent in English and dedicated to maintaining a global perspective, I\n",
      "üåé Notion (English)\n",
      "cixX1 pursued all my courses and curated my Notion exclusively in English\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    return hidden_states[-1].mean(dim=1).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ï∂îÍ∞Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embeddings = [get_embedding(paragraph) for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01163484, -0.03499083,  0.13591035, ...,  0.14383522,\n",
       "         0.02660656, -0.7701082 ],\n",
       "       [-0.5290672 , -0.6349337 ,  0.7247939 , ..., -0.6703838 ,\n",
       "         0.392003  , -0.1834573 ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,\n",
       " <faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x1094af180> >,\n",
       " array([[-0.01163484, -0.03499083,  0.13591035, ...,  0.14383522,\n",
       "          0.02660656, -0.7701082 ],\n",
       "        [-0.5290672 , -0.6349337 ,  0.7247939 , ..., -0.6703838 ,\n",
       "          0.392003  , -0.1834573 ]], dtype=float32))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "dimension, index, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Í≤ÄÏÉâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_question(question, index, model, k=3):\n",
    "    question_embedding = model.encode(question)\n",
    "    question_embedding = np.array([question_embedding]).astype('float32')\n",
    "    distances, indices = index.search(question_embedding, k)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_question_embedding(question, tokenizer, model):\n",
    "    inputs = tokenizer(question, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    # Assuming you want to use the last layer hidden state as the embedding\n",
    "    hidden_states = outputs.hidden_states[-1]\n",
    "    embeddings = hidden_states.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Define the search function\n",
    "def search_question(question, index, tokenizer, model, k=3):\n",
    "    question_embedding = get_question_embedding(question, tokenizer, model)\n",
    "    question_embedding = np.array([question_embedding]).astype('float32')\n",
    "    distances, indices = index.search(question_embedding, k)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# Example usage\n",
    "question = \"What certifications does Seul Kim have?\"\n",
    "top_k_indices = search_question(question, index, tokenizer, model)\n",
    "top_k_paragraphs = [paragraphs[i] for i in top_k_indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pdfplumber\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "\n",
    "# Function to load, split, and retrieve documents from a PDF file\n",
    "def load_and_retrieve_docs(file):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF file: {e}\"\n",
    "    \n",
    "    if not text:\n",
    "        return \"No text found in the PDF file.\"\n",
    "    \n",
    "    docs = [Document(page_content=text)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Function that defines the RAG chain\n",
    "def rag_chain(file, question):\n",
    "    retriever = load_and_retrieve_docs(file)\n",
    "    if isinstance(retriever, str):  # If an error message is returned\n",
    "        return retriever\n",
    "    \n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "    response = ollama.chat(model='llama3', \n",
    "                           messages=[\n",
    "                                {\"role\": \"system\",\n",
    "                                 \"content\": \"You are a helpful assistant. Check the pdf content and answer the question.\"\n",
    "                                },\n",
    "                                {\"role\": \"user\", \"content\": formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=rag_chain,\n",
    "    inputs=[\"file\", \"text\"],\n",
    "    outputs=\"text\",\n",
    "    title=\"LLAMA 3: RAG Chain Question Answering\",\n",
    "    description=\"Upload a PDF and enter a query to get answers from the RAG chain.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(file) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seul Kim\\nDesired position Data Scientist / Data Analyst (0 year experience)\\nEmail niceonesuri@gmail.com\\nSelf-driven learner\\nProactive problem solver\\nMobile (+82) 010-4415-3388\\nSolid team player\\nStrength\\nCloud Expertise ‚Ä¢ Certified AWS Cloud Practitioner with hands-on experience in various AWS services\\n‚Ä¢ Planning to be certified as the AWS Certified Data Engineer - Associate by early June\\nDatabase ‚Ä¢ Advanced proficiency in MySQL database management\\nManagement ‚Ä¢ Deep understanding of the differences between various AWS database services, such as DynamoDB,\\nAurora, and RDS, enabling informed decisions on selecting the most appropriate service based on\\nspecific use cases and requirements\\nData Analysis ‚Ä¢ Confident in applying AI/ML techniques to solve problems using TensorFlow & PyTorch\\n& Big Data ‚Ä¢ Skilled in handling large datasets and conducting complex data analysis and visualisation, using R,\\nPython, and Tableau, deriving actionable insights and enabling data-driven decisions\\nSoftware ‚Ä¢ Strong command-line skills in Linux OS for system administration and software deployment\\n& Web Dev ‚Ä¢ Proficient in Git/Github for streamlined version control\\n‚Ä¢ Experienced in full-stack web development processes using HTML/CSS/Boostrap5/JavaScript/Flask\\nEnglish ‚Ä¢ Fluent in both written and spoken English, confident to engage in international activities\\n& Soft Skills ‚Ä¢ A people person - ensuring happy customers, whilst working well with internal teams\\nCertifications\\nTOEIC (score : 970) Mar 16, 2024 YBM TOEIC\\nOPIC (level : AL) Feb 05, 2024 ACTFL OPIc\\nAWS CCP (Certified Cloud Practitioner) May 04, 2024 AWS (Amazon Web Service)\\nElementary Teacher Certification 1st : Feb 16, 2022 1st : Seoul Metropolitan Office of Education\\n(1st & 2nd Grade Certified Teacher) 2nd : Feb 24, 2017 2nd : Seoul National University of Education\\nBlogs\\nhttps://shorturl.at/ Fluent in English and dedicated to maintaining a global perspective, I\\nüåé Notion (English)\\ncixX1 pursued all my courses and curated my Notion exclusively in English',\n",
       " '.\\nhttps://smartest- Passionate about crafting essays and journals, I hope to contribute\\nüá∞üá∑ Tistory (Korean)\\nsuri.tistory.com/ tutorials and technical articles to the AWS community.\\nWork Experience\\nSeoul Bonghwa Sep 2022 ‚Ä¢ Managed the education welfare and funding database of the school in\\nElementary School - Feb 2024 collaboration with local administrative welfare centers, ensuring accurate and\\nefficient tracking of student support services and funding allocations.\\n‚Ä¢ Provided education for students in programming languages such as Python,\\nEntry, and Scratch, with the aim of future careers in technology.\\nSeoul Hwibong Sep 2018\\n‚Ä¢ Mentored students within the Python club - fun application projects of their\\nElementary School - Aug 2022\\nown helps stimulate an interest in the field.\\n‚Ä¢ Oversaw recruitment and supervision of both English language (native) and\\ncoding instructors for after-school programmes.Projects 01\\nSubject [Personal EDA project] AI Courseware trends analysis & its potential in computer vision\\nGithub URL https://github.com/surisurikim/EDA_personal_Education_AI\\nWhat I learned 1. Persistence and Problem-solving: Learned the importance of persistent digging to solve problems\\neffectively, and the spirit required to tackle challenges in troubleshooting technical inquiries.\\n2. Collaboration and Mentorship: Seeking advice from seniors and mentors, accepting their\\nopinions humbly enhanced the quality of the project, demonstrating a commitment to continuous\\nlearning and collaboration.\\n3. Continuous Learning and Growth: Challenging myself in various areas, including building CNN\\nmodels, provided invaluable learning opportunities and strengthened my proficiency in relevant\\ntechnologies. This aligns with the job responsibilities of learning groundbreaking technologies to\\nprovide innovative solutions to customers.\\nProjects 02\\nSubject [Front-end Web developing] Swipe Website & Yavin Website projects\\nGithub URL https://github.com/surisurikim/swipe_project\\nhttps://github.com/surisurikim/boostrap5_project\\nWhat I learned 1. Web Technology Proficiency: Hands-on experience with HTML, CSS, JavaScript, and\\nBootstrap5 strengthened my front-end web development skills, readying me to tackle customer\\nservice challenges with web solutions.\\n2. User-Centric Approach: Project experiences deepened understanding of user experience design,\\nempowering me to craft web solutions tailored to meet customer needs promptly and adeptly.\\nProjects 03\\nSubject Developed a model to predict engagement rates for YouTube beauty advertisements using various\\ndata processing and modeling techniques.\\nGithub URL (Preparing)\\nWhat I learned ‚Ä¢ Advanced Data Handling: Mastered data collection, cleaning, and preprocessing using APIs like\\nYouTube Data API, AWS Rekognition, OpenAI, DeepL, and Google Translation.\\n‚Ä¢ Team Leadership: Strengthened leadership skills by managing a team, guiding coding practices,\\nand supervising the creation of presentation materials.\\n‚Ä¢ Analytical Techniques: Enhanced ability in exploratory data analysis (EDA), modeling,\\nhyperparameter tuning, and deriving actionable insights from data.\\nEducational Qualifications\\nCyber University of Korea Mar 2024 - Present ‚Ä¢ Transferred as a third-year Software Engineering student\\nASA College, New York, USA Mar 2022 - Aug 2022 ‚Ä¢ Participated in the IEP (Intensive English Programme)\\nsponsored by Seoul Metropolitan Office of Education\\nSeoul National University Mar 2013 - Feb 2017 ‚Ä¢ B/A in Elementary Education (GPA 2.98/4.5)\\nof Education ‚Ä¢ Specialisation in Computer Education\\nI confirm that all the information provided above is accurate.\\nSeul Kim',\n",
       " '.\\nhttps://smartest- Passionate about crafting essays and journals, I hope to contribute\\nüá∞üá∑ Tistory (Korean)\\nsuri.tistory.com/ tutorials and technical articles to the AWS community.\\nWork Experience\\nSeoul Bonghwa Sep 2022 ‚Ä¢ Managed the education welfare and funding database of the school in\\nElementary School - Feb 2024 collaboration with local administrative welfare centers, ensuring accurate and\\nefficient tracking of student support services and funding allocations.\\n‚Ä¢ Provided education for students in programming languages such as Python,\\nEntry, and Scratch, with the aim of future careers in technology.\\nSeoul Hwibong Sep 2018\\n‚Ä¢ Mentored students within the Python club - fun application projects of their\\nElementary School - Aug 2022\\nown helps stimulate an interest in the field.\\n‚Ä¢ Oversaw recruitment and supervision of both English language (native) and\\ncoding instructors for after-school programmes.Projects 01\\nSubject [Personal EDA project] AI Courseware trends analysis & its potential in computer vision\\nGithub URL https://github.com/surisurikim/EDA_personal_Education_AI\\nWhat I learned 1. Persistence and Problem-solving: Learned the importance of persistent digging to solve problems\\neffectively, and the spirit required to tackle challenges in troubleshooting technical inquiries.\\n2. Collaboration and Mentorship: Seeking advice from seniors and mentors, accepting their\\nopinions humbly enhanced the quality of the project, demonstrating a commitment to continuous\\nlearning and collaboration.\\n3. Continuous Learning and Growth: Challenging myself in various areas, including building CNN\\nmodels, provided invaluable learning opportunities and strengthened my proficiency in relevant\\ntechnologies. This aligns with the job responsibilities of learning groundbreaking technologies to\\nprovide innovative solutions to customers.\\nProjects 02\\nSubject [Front-end Web developing] Swipe Website & Yavin Website projects\\nGithub URL https://github.com/surisurikim/swipe_project\\nhttps://github.com/surisurikim/boostrap5_project\\nWhat I learned 1. Web Technology Proficiency: Hands-on experience with HTML, CSS, JavaScript, and\\nBootstrap5 strengthened my front-end web development skills, readying me to tackle customer\\nservice challenges with web solutions.\\n2. User-Centric Approach: Project experiences deepened understanding of user experience design,\\nempowering me to craft web solutions tailored to meet customer needs promptly and adeptly.\\nProjects 03\\nSubject Developed a model to predict engagement rates for YouTube beauty advertisements using various\\ndata processing and modeling techniques.\\nGithub URL (Preparing)\\nWhat I learned ‚Ä¢ Advanced Data Handling: Mastered data collection, cleaning, and preprocessing using APIs like\\nYouTube Data API, AWS Rekognition, OpenAI, DeepL, and Google Translation.\\n‚Ä¢ Team Leadership: Strengthened leadership skills by managing a team, guiding coding practices,\\nand supervising the creation of presentation materials.\\n‚Ä¢ Analytical Techniques: Enhanced ability in exploratory data analysis (EDA), modeling,\\nhyperparameter tuning, and deriving actionable insights from data.\\nEducational Qualifications\\nCyber University of Korea Mar 2024 - Present ‚Ä¢ Transferred as a third-year Software Engineering student\\nASA College, New York, USA Mar 2022 - Aug 2022 ‚Ä¢ Participated in the IEP (Intensive English Programme)\\nsponsored by Seoul Metropolitan Office of Education\\nSeoul National University Mar 2013 - Feb 2017 ‚Ä¢ B/A in Elementary Education (GPA 2.98/4.5)\\nof Education ‚Ä¢ Specialisation in Computer Education\\nI confirm that all the information provided above is accurate.\\nSeul Kim']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
